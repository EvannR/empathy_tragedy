{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_game_theoretic import GameTheoreticEnv\n",
    "from agent_policies_game_theoretic import QAgent, DQNAgent\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_to_csv_episode_data(states_per_step, filename='simulation_data.csv'):\n",
    "    \"\"\"\n",
    "    Export episode data to CSV with one line per step, including for each agent:\n",
    "    - current resource level\n",
    "    - observation (emotion)\n",
    "    - action selected\n",
    "    - personal_reward\n",
    "    - empathic_reward\n",
    "    - combined_reward (total internal reward)\n",
    "    \"\"\"\n",
    "    import csv\n",
    "\n",
    "    # Number of agents\n",
    "    n_agents = len(states_per_step[0]['observations'])\n",
    "\n",
    "    # Dynamically construct CSV headers\n",
    "    fieldnames = ['step', 'resource']\n",
    "    for i in range(n_agents):\n",
    "        fieldnames += [\n",
    "            f'observation_{i}',\n",
    "            f'action_{i}',\n",
    "            f'personal_reward_{i}',\n",
    "            f'empathic_reward_{i}',\n",
    "            f'combined_reward_{i}'\n",
    "        ]\n",
    "\n",
    "    with open(filename, mode='w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for step_data in states_per_step:\n",
    "            row = {'step': step_data['step'], 'resource': step_data.get('resource', None)}\n",
    "            obs = step_data['observations']\n",
    "            acts = step_data['actions']\n",
    "            personal = step_data['personal_reward']\n",
    "            empathic = step_data['empathic_reward']\n",
    "            # combined reward stored as 'internal_total_reward'\n",
    "            combined = step_data.get('combined_reward', [])\n",
    "\n",
    "            for i in range(n_agents):\n",
    "                row[f'observation_{i}'] = obs[i]\n",
    "                row[f'action_{i}'] = acts[i]\n",
    "                row[f'personal_reward_{i}'] = personal[i]\n",
    "                row[f'empathic_reward_{i}'] = empathic[i]\n",
    "                # safe fallback to 0 if missing\n",
    "                row[f'combined_reward_{i}'] = combined[i] if i < len(combined) else None\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "    return filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe3b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_resource_evolution(states_per_step, env, save_path=\"resource_evolution.png\"):\n",
    "    \"\"\"\n",
    "    Generate a static image to visualize the evolution of the ressources\n",
    "    \"\"\"\n",
    "    steps = [step['step'] for step in states_per_step]\n",
    "    resources = [step['resource'] for step in states_per_step]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(steps, resources, label='Level of ressources', color='green', linewidth=2)\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Ressource\")\n",
    "    plt.title(f\"Fluctuation of ressources in the environment for agent: {agent_to_test} with empathy level: {alpha} and valuation of last meal: {beta}\")\n",
    "    plt.ylim(0, env.initial_resources * 1.1)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_q_table_detailed_to_csv(agents, filename=\"q_table_detailed.csv\"):\n",
    "    \"\"\"\n",
    "    Save each Q-value individually with action separation.\n",
    "    CSV format: agent_id, state, action, expected_reward\n",
    "    \"\"\"\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"agent_id\", \"state\", \"action\", \"expected_reward\"])\n",
    "\n",
    "        for agent_idx, agent in enumerate(agents):\n",
    "            if hasattr(agent, 'q_table'):  # QAgent only\n",
    "                for state, actions in agent.q_table.items():\n",
    "                    for action, value in enumerate(actions):\n",
    "                        writer.writerow([agent_idx, state, action, value])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f13acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_q_table(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    agent_id = 0\n",
    "    df_agent = df[df['agent_id'] == agent_id]\n",
    "\n",
    "    pivot_table = df_agent.pivot(index='state', columns='action', values='expected_reward')\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(f\"Q-table (agent {agent_id})\")\n",
    "    heatmap = plt.imshow(pivot_table.fillna(0), cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(heatmap, label='Expected Reward')\n",
    "    plt.xlabel(\"Action\")\n",
    "    plt.ylabel(\"State\")\n",
    "    plt.xticks(ticks=range(len(pivot_table.columns)), labels=pivot_table.columns)\n",
    "    plt.yticks(ticks=range(len(pivot_table.index)), labels=pivot_table.index)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c58229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filename_definer(agent_type,\n",
    "                     episode_number,\n",
    "                     emotion_type,\n",
    "                     see_emotions,\n",
    "                     alpha,\n",
    "                     beta,\n",
    "                     smoothing_type,\n",
    "                     threshold_value,\n",
    "                     emotion_rounder,\n",
    "                     params_DQN,\n",
    "                     params_QLearning):\n",
    "    \"\"\"\n",
    "    name of the file order : \n",
    "    episode number\n",
    "    agent_to_test = \"DQN\" or \"QLearning\"\n",
    "    emotion_type = can be \"average\" or \"vector\"\n",
    "    see_emotions = \"False\" or \"True\"\n",
    "    alpha = 1  # parameter for the degree of empathy (the higher the value the higher the empathy in range 0 - 1)\n",
    "    beta = 0.3 # valuation of the last meal\n",
    "    smoothing_type = linear or sigmoid\n",
    "    threshold_value  proportion of reward in the history necessary to have a positive emotion\n",
    "    emotion_rounder = decimale of emotions\n",
    "\n",
    "    the parameters of the agents are in the order :\n",
    "Params_QL\n",
    "    \"learning_rate\"\n",
    "    \"gamma\"\n",
    "    \"epsilon\"\n",
    "    \"epsilon_decay\"\n",
    "    \"epsilon_min\"\n",
    "\n",
    "params_DQN =\n",
    "    \"learning_rate\"\n",
    "    \"gamma\"\n",
    "    \"epsilon\"\n",
    "    \"epsilon_decay\"\n",
    "    \"epsilon_min\"\n",
    "    \"batch_size\"\n",
    "    \"hidden_size\"\n",
    "    \"update_target_every\"\n",
    "\n",
    "    return the filename of one episode with a random 6 int suffix\n",
    "    \"\"\"\n",
    "    if agent_type == \"DQN\":\n",
    "        params = params_DQN\n",
    "        param_order = [\"learning_rate\", \"gamma\", \"epsilon\", \"epsilon_decay\", \"epsilon_min\", \"batch_size\", \"hidden_size\", \"update_target_every\"]\n",
    "    elif agent_type == \"QLearning\":\n",
    "        params = params_QLearning\n",
    "        param_order = [\"learning_rate\", \"gamma\", \"epsilon\", \"epsilon_decay\", \"epsilon_min\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent type: {agent_type!r}\")\n",
    "\n",
    "    # Ensure values appear in fixed order (no key names)\n",
    "    param_values = \"_\".join(str(params[key]) for key in param_order)\n",
    "\n",
    "    random_suffix = ''.join(str(random.randint(0, 9)) for _ in range(6))\n",
    "    see_emotions_str = str(see_emotions)\n",
    "\n",
    "    filename = (\n",
    "        f\"results_\"\n",
    "        f\"{episode_number}_\"\n",
    "        f\"{agent_type}_\"\n",
    "        f\"{emotion_type}_\"\n",
    "        f\"{see_emotions_str}_\"\n",
    "        f\"{alpha}_\"\n",
    "        f\"{beta}_\"\n",
    "        f\"{smoothing_type}_\"\n",
    "        f\"{threshold_value}_\"\n",
    "        f\"{emotion_rounder}_\"\n",
    "        f\"{param_values}_\"\n",
    "        f\"{random_suffix}.csv\"\n",
    "    )\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for episode_number in range(1, episodes+1):\n",
    "        states, env, agents = run_simulation()\n",
    "        filename_data = export_to_csv_episode_data(states,\n",
    "                                                   filename=filename_definer(agent_type=agent_to_test,\n",
    "                                                                             episode_number=episode_number,\n",
    "                                                                             emotion_type=emotion_type,\n",
    "                                                                             see_emotions=see_emotions,\n",
    "                                                                             alpha=alpha,\n",
    "                                                                             beta=beta,\n",
    "                                                                             smoothing_type=smoothing_type,\n",
    "                                                                             threshold_value=threshold_value,\n",
    "                                                                             emotion_rounder=emotion_rounder,\n",
    "                                                                             params_DQN=params_DQN,\n",
    "                                                                             params_QLearning=params_QLearning\n",
    "                                                                             )\n",
    "                                                        )\n",
    "\n",
    "        plot_resource_evolution(states,\n",
    "                                env)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
