{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f5d9d8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2f8ffec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda237a",
   "metadata": {},
   "source": [
    "# General variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b3f962",
   "metadata": {},
   "source": [
    " ## Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d883acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern of filenames\n",
    "\n",
    "GameTheoretic_filename_pattern_DQN =  re.compile(r\"results_(?P<simulation_index>\\d{3})_(?P<episodes>\\d+)_DQN_\"\n",
    "                                                r\"(?P<emotion>[^_]+)_(?P<see_emotions>[^_]+)_\"\n",
    "                                                r\"(?P<alpha>[\\d.]+)_(?P<beta>[\\d.]+)_(?P<smoothing>[^_]+)_(?P<threshold>[\\d.]+)_(?P<rounder>[\\d.]+)_\"\n",
    "                                                r\"(?P<learning_rate>[\\d.]+)_(?P<gamma>[\\d.]+)_(?P<epsilon>[\\d.]+)_(?P<epsilon_decay>[\\d.]+)_(?P<epsilon_min>[\\d.]+)_\"\n",
    "                                                r\"(?P<batch_size>[\\d.]+)_(?P<hidden_size>[\\d.]+)_(?P<update_target_every>[\\d.]+)_\"\n",
    "                                                r\"(?P<random_suffix>\\d{6})_(?P<suffix>[a-zA-Z]+_[a-zA-Z]+)\\.csv\"\n",
    ")\n",
    "\n",
    "GameTheoretic_filename_pattern_QL = re.compile(r\"results_(?P<simulation_index>\\d{3})_(?P<episodes>\\d+)_QLearning_\"\n",
    "                                              r\"(?P<emotion>[^_]+)_(?P<see_emotions>[^_]+)_\"\n",
    "                                              r\"(?P<alpha>[\\d.]+)_(?P<beta>[\\d.]+)_(?P<smoothing>[^_]+)_(?P<threshold>[\\d.]+)_(?P<rounder>[\\d.]+)_\"\n",
    "                                              r\"(?P<learning_rate>[\\d.]+)_(?P<gamma>[\\d.]+)_(?P<epsilon>[\\d.]+)_(?P<epsilon_decay>[\\d.]+)_(?P<epsilon_min>[\\d.]+)_\"\n",
    "                                              r\"(?P<random_suffix>\\d{6})_(?P<suffix>[a-zA-Z]+_[a-zA-Z]+)\\.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "Maze2D_filename_order_QL = re.compile(\n",
    "    r\"maze2d_results_(?P<simulation_index>\\d{3})_(?P<episodes>\\d+)_QLearning_\"\n",
    "    r\"(?P<emotion>[^_]+)_(?P<see_emotions>[^_]+)_\"\n",
    "    r\"(?P<alpha>[\\d.]+)_(?P<beta>[\\d.]+)_(?P<smoothing>[^_]+)_(?P<threshold>[\\d.]+)_(?P<rounder>[\\d.]+)_\"\n",
    "    r\"(?P<learning_rate>[\\d.]+)_(?P<gamma>[\\d.]+)_(?P<epsilon>[\\d.]+)_(?P<epsilon_decay>[\\d.]+)_(?P<epsilon_min>[\\d.]+)_\"\n",
    "    r\"(?P<random_suffix>\\d{6})_(?P<suffix>[a-zA-Z]+_[a-zA-Z]+)\\.csv\"\n",
    ")\n",
    "\n",
    "Maze2D_filename_order_DQN = re.compile(\n",
    "    r\"maze2d_results_(?P<simulation_index>\\d{3})_(?P<episodes>\\d+)_DQN_\"\n",
    "    r\"(?P<emotion>[^_]+)_(?P<see_emotions>[^_]+)_\"\n",
    "    r\"(?P<alpha>[\\d.]+)_(?P<beta>[\\d.]+)_(?P<smoothing>[^_]+)_(?P<threshold>[\\d.]+)_(?P<rounder>[\\d.]+)_\"\n",
    "    r\"(?P<learning_rate>[\\d.]+)_(?P<gamma>[\\d.]+)_(?P<epsilon>[\\d.]+)_(?P<epsilon_decay>[\\d.]+)_(?P<epsilon_min>[\\d.]+)_\"\n",
    "    r\"(?P<batch_size>[\\d.]+)_(?P<hidden_size>[\\d.]+)_(?P<update_target_every>[\\d.]+)_\"\n",
    "    r\"(?P<random_suffix>\\d{6})_(?P<suffix>[a-zA-Z]+_[a-zA-Z]+)\\.csv\"\n",
    ")\n",
    "\n",
    "FILENAME_PATTERNS = [\n",
    "    GameTheoretic_filename_pattern_DQN,\n",
    "    GameTheoretic_filename_pattern_QL,\n",
    "    Maze2D_filename_order_DQN,\n",
    "    Maze2D_filename_order_QL\n",
    "]\n",
    "\n",
    "FILENAME_PATTERNS_PAIR = [\n",
    "    (\"Gametheoretic\", GameTheoretic_filename_pattern_DQN),\n",
    "    (\"Gametheoretic\", GameTheoretic_filename_pattern_QL),\n",
    "    (\"maze2d\", Maze2D_filename_order_DQN),\n",
    "    (\"maze2d\", Maze2D_filename_order_QL)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810fa323",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d4083a",
   "metadata": {},
   "source": [
    "## CSV processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad5067",
   "metadata": {},
   "source": [
    "### Parameter recovery from filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2748031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def parse_results_filenames(folder_path: str, filename_patterns=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scans a folder for result filenames and extracts simulation parameters into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing result CSV files.\n",
    "        filename_patterns (list): List of compiled regex patterns to match filenames.\n",
    "                                  If None, use global FILENAME_PATTERNS.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing parsed parameters from filenames.\n",
    "    \"\"\"\n",
    "    if filename_patterns is None:\n",
    "        filename_patterns = FILENAME_PATTERNS\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        matched = False\n",
    "        for pattern in filename_patterns:\n",
    "            match = pattern.match(filename)\n",
    "            if match:\n",
    "                file_data = match.groupdict()\n",
    "                file_data[\"filename\"] = filename\n",
    "                data.append(file_data)\n",
    "                matched = True\n",
    "                break  # Stop at the first match\n",
    "        \n",
    "        if not matched:\n",
    "            print(f\"Warning: filename did not match any pattern: {filename}\")\n",
    "\n",
    "    if not data:\n",
    "        print(\"No matching filenames found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Optional: convert numeric fields from str to float/int\n",
    "    for col in df.columns:\n",
    "        if col not in {\"filename\", \"emotion\", \"see_emotions\", \"suffix\"}:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col])\n",
    "            except Exception:\n",
    "                pass  # leave as string if conversion fails\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d5b8ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unique_parameter_values(df: pd.DataFrame, exclude: list = None):\n",
    "    \"\"\"\n",
    "    Print a table with parameter names and their unique values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with simulation parameters.\n",
    "        exclude (list): Optional list of column names to exclude (e.g., ['filename', 'simulation_index']).\n",
    "    \"\"\"\n",
    "    if exclude is None:\n",
    "        exclude = ['filename', 'simulation_index']\n",
    "\n",
    "    param_cols = [col for col in df.columns if col not in exclude]\n",
    "\n",
    "    summary = {\n",
    "        \"parameter\": [],\n",
    "        \"unique_values\": []\n",
    "    }\n",
    "\n",
    "    for col in param_cols:\n",
    "        summary[\"parameter\"].append(col)\n",
    "        summary[\"unique_values\"].append(sorted(df[col].dropna().unique().tolist()))\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    print(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd5844",
   "metadata": {},
   "source": [
    "### Aggregation of csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ffa3efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results_by_suffix(folder_path: str, target_suffix: str, source_filter: str = None) -> pd.DataFrame:\n",
    "    all_data = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        for source_type, pattern in FILENAME_PATTERNS_PAIR:\n",
    "            if source_filter and source_type.lower() != source_filter.lower():\n",
    "                continue\n",
    "\n",
    "            match = pattern.match(filename)\n",
    "            if match:\n",
    "                metadata = match.groupdict()\n",
    "                if metadata.get(\"suffix\", \"\").strip() == target_suffix.strip():\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        for key, value in metadata.items():\n",
    "                            df[key] = value\n",
    "                        df[\"source\"] = source_type.lower()  # normalize\n",
    "                        all_data.append(df)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {filename}: {e}\")\n",
    "                break\n",
    "\n",
    "    if not all_data:\n",
    "        print(f\"No matching files found for suffix '{target_suffix}' and source '{source_filter}'.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    for col in final_df.columns:\n",
    "        if col not in {\"emotion\", \"see_emotions\", \"suffix\", \"filename\", \"source\"}:\n",
    "            try:\n",
    "                final_df[col] = pd.to_numeric(final_df[col])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    filtered_tag = f\"_{source_filter.lower()}\" if source_filter else \"\"\n",
    "    output_filename = f\"aggregated_{target_suffix}{filtered_tag}.csv\"\n",
    "    output_path = os.path.join(folder_path, output_filename)\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved aggregated data to: {output_path}\")\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16bd09d",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b159a",
   "metadata": {},
   "source": [
    " ### Learning verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "89ec1a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def windowed_avg_combined_reward(\n",
    "    df: pd.DataFrame,\n",
    "    reward_prefix: str = \"total_combined_reward_\",\n",
    "    episode_column: str = \"episode\",\n",
    "    simulation_id_column: str = \"simulation_index\",\n",
    "    window_size: int = 5,\n",
    "    aggregation_mode: str = \"mean\",  # or \"best\"\n",
    "    plot: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes a windowed moving average of combined rewards per episode across simulations.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        reward_prefix (str): Prefix of reward columns per agent.\n",
    "        episode_column (str): Column name for episodes.\n",
    "        simulation_id_column (str): Column indicating different simulations.\n",
    "        window_size (int): Window size for moving average.\n",
    "        aggregation_mode (str): 'mean' for average across agents, 'best' for max reward among agents.\n",
    "        plot (bool): Whether to plot the result.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with ['episode', 'aggregated_reward', 'moving_avg'].\n",
    "    \"\"\"\n",
    "    reward_cols = [col for col in df.columns if col.startswith(reward_prefix)]\n",
    "    if not reward_cols:\n",
    "        raise ValueError(f\"No columns found with prefix '{reward_prefix}'\")\n",
    "\n",
    "    if aggregation_mode == \"mean\":\n",
    "        df[\"aggregated_reward\"] = df[reward_cols].mean(axis=1)\n",
    "    elif aggregation_mode == \"best\":\n",
    "        df[\"aggregated_reward\"] = df[reward_cols].max(axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"aggregation_mode must be 'mean' or 'best'\")\n",
    "\n",
    "    episode_avg = (\n",
    "        df.groupby(episode_column)[\"aggregated_reward\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"aggregated_reward\": \"mean_reward\"})\n",
    "    )\n",
    "\n",
    "    # Apply moving average\n",
    "    episode_avg[\"moving_avg\"] = (\n",
    "        episode_avg[\"mean_reward\"].rolling(window=window_size, min_periods=1, center=True).mean()\n",
    "    )\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(episode_avg[episode_column], episode_avg[\"moving_avg\"], label=f\"Moving Avg ({aggregation_mode})\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(f\"{aggregation_mode.capitalize()} Agent Reward (Window={window_size})\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return episode_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96180ba",
   "metadata": {},
   "source": [
    "### Variable_calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db0720",
   "metadata": {},
   "source": [
    "#### Gini coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "da6bcc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_coefficient(arr: np.ndarray) -> float:\n",
    "    \"\"\"Compute Gini coefficient of a 1D numpy array.\"\"\"\n",
    "    arr = arr.flatten()\n",
    "    if np.amin(arr) < 0:\n",
    "        arr = arr - np.amin(arr)  # Shift if negative values present\n",
    "    mean = np.mean(arr)\n",
    "    if mean == 0:\n",
    "        return 0.0\n",
    "    n = len(arr)\n",
    "    diff_sum = np.sum(np.abs(np.subtract.outer(arr, arr)))\n",
    "    gini = diff_sum / (2 * n**2 * mean)\n",
    "    return gini\n",
    "\n",
    "def compute_gini_for_df(df: pd.DataFrame, prefix: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute Gini coefficient across columns starting with prefix for each row in df.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame.\n",
    "        prefix: string prefix for target columns.\n",
    "        \n",
    "    Returns:\n",
    "        pandas Series with Gini coefficients per row.\n",
    "    \"\"\"\n",
    "    cols = [col for col in df.columns if col.startswith(prefix)]\n",
    "    if not cols:\n",
    "        raise ValueError(f\"No columns found starting with prefix '{prefix}'\")\n",
    "\n",
    "    gini_series = df[cols].apply(lambda row: gini_coefficient(row.values), axis=1)\n",
    "    return gini_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a73761",
   "metadata": {},
   "source": [
    "#### Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d9c00a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_efficiency_for_df(df: pd.DataFrame, prefix: str, new_column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the average across columns starting with a given prefix for each row in df.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        prefix (str): Prefix for selecting target columns.\n",
    "        new_column_name (str): Name of the new column to store the computed sum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with the new column added.\n",
    "    \"\"\"\n",
    "    cols = [col for col in df.columns if col.startswith(prefix)]\n",
    "    if not cols:\n",
    "        raise ValueError(f\"No columns found starting with prefix '{prefix}'\")\n",
    "\n",
    "    df[new_column_name] = df[cols].mean(axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7ad29",
   "metadata": {},
   "source": [
    "#### Ressource depletion : to discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7eae9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _clean_initial_resources_column(df: pd.DataFrame, col_name: str) -> None: # Added because sometimes they were str rather than int\n",
    "    \"\"\"Convert list-like string in initial_resources column to numeric.\"\"\"\n",
    "    def extract_number(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            if isinstance(parsed, (list, tuple)) and len(parsed) > 0:\n",
    "                return float(parsed[0])\n",
    "            else:\n",
    "                return np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    if col_name in df.columns and df[col_name].dtype == object:\n",
    "        df[col_name] = df[col_name].apply(extract_number)\n",
    "        df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "\n",
    "def GT_compute_and_merge_depletion_from_step(df: pd.DataFrame) -> None:\n",
    "    _clean_initial_resources_column(df, 'initial_resources')\n",
    "\n",
    "    group_cols = ['simulation_index', 'episode']\n",
    "    grouped = df.groupby(group_cols)\n",
    "\n",
    "    depletion_metrics = grouped.agg(\n",
    "        final_resource=('resource_remaining', 'last'),\n",
    "        avg_resource=('resource_remaining', 'mean'),\n",
    "        initial_resource=('initial_resources', 'first')\n",
    "    ).reset_index()\n",
    "\n",
    "    depletion_metrics['depletion_final'] = 1 - depletion_metrics['final_resource'] / depletion_metrics['initial_resource']\n",
    "    depletion_metrics['depletion_cumulative'] = 1 - depletion_metrics['avg_resource'] / depletion_metrics['initial_resource']\n",
    "\n",
    "    merged = df.merge(\n",
    "        depletion_metrics[group_cols + ['depletion_final', 'depletion_cumulative']],\n",
    "        on=group_cols, how='left'\n",
    "    )\n",
    "\n",
    "    df.loc[:, 'depletion_final'] = merged['depletion_final']\n",
    "    df.loc[:, 'depletion_cumulative'] = merged['depletion_cumulative']\n",
    "\n",
    "\n",
    "def GT_compute_depletion_from_summary(df: pd.DataFrame) -> None:\n",
    "    # Clean initial_resources column if needed\n",
    "    _clean_initial_resources_column(df, 'initial_resources')\n",
    "\n",
    "    df.loc[:, 'depletion_final'] = 1 - df['resource_remaining'] / df['initial_resources']\n",
    "    df.loc[:, 'depletion_early'] = 1 - df['total_steps'] / df['max_steps']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e56d8",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0277a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the fluctuation of a value over episodes or steps\n",
    "def plot_value_fluctuation_by_simulation(\n",
    "    df: pd.DataFrame,\n",
    "    value_col: str, # name of the column over which we study the fluctuation\n",
    "    simulation_col: str = 'seed',\n",
    "    episode_col: str = 'episode',\n",
    "    step_col: str = 'step',\n",
    "    is_step_csv: bool = False,\n",
    "    title: str = None,\n",
    "    ylabel: str = None,\n",
    "    rolling_window: int = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the fluctuation of a value averaged per simulation over episodes (summary) or (step, episode) (step CSV).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame (step or summary).\n",
    "        value_col (str): The column to track (e.g. 'resource_remaining', 'reward').\n",
    "        simulation_col (str): Column identifying the simulation (default 'seed').\n",
    "        episode_col (str): Column identifying the episode (default 'episode').\n",
    "        step_col (str): Column identifying the step (default 'step').\n",
    "        is_step_csv (bool): True if using step CSV; False if using summary CSV.\n",
    "        title (str): Optional plot title.\n",
    "        ylabel (str): Label for y-axis (default = value_col).\n",
    "        rolling_window (int): Optional window for smoothing (rolling mean).\n",
    "    \"\"\"\n",
    "    if is_step_csv:\n",
    "        group_cols = [simulation_col, episode_col, step_col]\n",
    "    else:\n",
    "        group_cols = [simulation_col, episode_col]\n",
    "\n",
    "    grouped = df.groupby(group_cols)[value_col].mean().reset_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for sim_id, sim_df in grouped.groupby(simulation_col):\n",
    "        x = sim_df[episode_col] if not is_step_csv else sim_df.groupby([episode_col])[step_col].apply(list)\n",
    "        y = sim_df[value_col]\n",
    "\n",
    "        if rolling_window:\n",
    "            y = y.rolling(rolling_window, min_periods=1).mean()\n",
    "\n",
    "        label = f\"Sim {sim_id}\"\n",
    "        plt.plot(sim_df[episode_col] if not is_step_csv else range(len(y)), y, label=label)\n",
    "\n",
    "    plt.xlabel('Episode' if not is_step_csv else 'Time (by episode/step)')\n",
    "    plt.ylabel(ylabel if ylabel else value_col)\n",
    "    plt.title(title or f\"Fluctuation of {value_col} over time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "86be37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# density function of the depletion per episode\n",
    "def plot_depletion_density_across_simulations(\n",
    "    df: pd.DataFrame,\n",
    "    depletion_col: str = 'depletion_final',\n",
    "    episode_col: str = 'episode',\n",
    "    method: str = 'kde',  # 'kde' or 'hist'\n",
    "    bandwidth: float = 0.3,\n",
    "    bins: int = 30,\n",
    "    title: str = \"Resource Depletion Density Across Simulations\",\n",
    "    figsize: tuple = (12, 6)\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a 2D density or histogram of depletion values across all simulations over episodes.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing at least 'episode' and 'depletion_final'.\n",
    "        depletion_col (str): Column for resource depletion.\n",
    "        episode_col (str): Column for episode index.\n",
    "        method (str): 'kde' for smoothed density, 'hist' for 2D histogram.\n",
    "        bandwidth (float): Smoothing parameter for KDE (ignored for hist).\n",
    "        bins (int): Number of bins for histogram (ignored for KDE).\n",
    "        title (str): Plot title.\n",
    "        figsize (tuple): Size of the figure.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    if method == 'kde':\n",
    "        sns.kdeplot(\n",
    "            data=df,\n",
    "            x=episode_col,\n",
    "            y=depletion_col,\n",
    "            fill=True,\n",
    "            cmap=\"mako\",\n",
    "            bw_adjust=bandwidth,\n",
    "            levels=100,\n",
    "            thresh=0.05\n",
    "        )\n",
    "    elif method == 'hist':\n",
    "        sns.histplot(\n",
    "            data=df,\n",
    "            x=episode_col,\n",
    "            y=depletion_col,\n",
    "            bins=bins,\n",
    "            pmax=0.95,\n",
    "            cbar=True,\n",
    "            cmap=\"mako\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'kde' or 'hist'\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Episode Number\")\n",
    "    plt.ylabel(\"Resource Depletion\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0dc967",
   "metadata": {},
   "source": [
    "# Analysis of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "59966418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: filename did not match any pattern: aggregated_episode_summary_gametheoretic.csv\n",
      "Warning: filename did not match any pattern: aggregated_step_data_gametheoretic.csv\n",
      "emotion see_emotions  alpha  beta smoothing  threshold  learning_rate  gamma  epsilon  epsilon_decay  epsilon_min  batch_size  hidden_size  update_target_every\n",
      "average         True    0.5   0.5    linear        0.5          0.001   0.99      1.0          0.995         0.01          16           64                    5\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Gather metadata from filenames ---\n",
    "folder_path = \".\"  # Folder containing the CSV results\n",
    "\n",
    "parameter_dataframe = parse_results_filenames(folder_path=folder_path)\n",
    "\n",
    "# Define the specific RL parameters of interest\n",
    "desired_params = [\n",
    "    \"emotion\", \"see_emotions\", \"alpha\", \"beta\", \"smoothing\", \"threshold\",\n",
    "    \"learning_rate\", \"gamma\", \"epsilon\", \"epsilon_decay\", \"epsilon_min\",\n",
    "    \"batch_size\", \"hidden_size\", \"update_target_every\"\n",
    "]\n",
    "\n",
    "param_cols = [col for col in parameter_dataframe.columns if col in desired_params]\n",
    "unique_values_per_param = {col: sorted(parameter_dataframe[col].unique()) for col in param_cols}\n",
    "\n",
    "# Convert to DataFrame with param names as columns and unique set of values as rows\n",
    "max_len = max(len(v) for v in unique_values_per_param.values())\n",
    "for k in unique_values_per_param:\n",
    "    unique_values_per_param[k] += [None] * (max_len - len(unique_values_per_param[k]))\n",
    "\n",
    "# Display of the unique sets of parameter values\n",
    "summary_df = pd.DataFrame(unique_values_per_param)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "24ed8d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching files found for suffix 'episode_summary' and source 'maze2d'.\n",
      "No matching files found for suffix 'step_data' and source 'maze2d'.\n",
      "Saved aggregated data to: .\\aggregated_episode_summary_gametheoretic.csv\n",
      "Saved aggregated data to: .\\aggregated_step_data_gametheoretic.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Aggregate data for Maze2D ---\n",
    "df_maze_summary = aggregate_results_by_suffix(\n",
    "    folder_path=folder_path,\n",
    "    target_suffix=\"episode_summary\",\n",
    "    source_filter=\"maze2d\"\n",
    ")\n",
    "\n",
    "df_maze_step = aggregate_results_by_suffix(\n",
    "    folder_path=folder_path,\n",
    "    target_suffix=\"step_data\",\n",
    "    source_filter=\"maze2d\"\n",
    ")\n",
    "\n",
    "# --- Aggregate data for GameTheoretic environment only ---\n",
    "df_gt_summary = aggregate_results_by_suffix(\n",
    "    folder_path=folder_path,\n",
    "    target_suffix=\"episode_summary\",\n",
    "    source_filter=\"Gametheoretic\"\n",
    ")\n",
    "\n",
    "df_gt_step = aggregate_results_by_suffix(\n",
    "    folder_path=folder_path,\n",
    "    target_suffix=\"step_data\",\n",
    "    source_filter=\"Gametheoretic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aea3e3",
   "metadata": {},
   "source": [
    "## Calculation of Dependent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6842deed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 32)\n",
      "(17903, 34)\n"
     ]
    }
   ],
   "source": [
    "print(df_gt_summary.shape)\n",
    "print(df_gt_step.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "98825e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Could not compute Gini for df_maze_summary: No columns found starting with prefix 'total_personal_reward_'\n",
      "[]\n",
      "Could not compute Gini for df_maze_step: No columns found starting with prefix 'reward_'\n",
      "succesfull for GameTheoretic summary\n",
      "succesfull for GameTheoretic step\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print([col for col in df_maze_summary.columns if col.startswith(\"total_personal_reward_\")])\n",
    "    df_maze_summary[\"gini_personal_reward\"] = compute_gini_for_df(df_maze_summary, prefix=\"total_personal_reward_\")\n",
    "    print('succesfull for 2D summary')\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute Gini for df_maze_summary: {e}\")\n",
    "\n",
    "try:\n",
    "    print([col for col in df_maze_step.columns if col.startswith(\"reward_\")])\n",
    "    df_maze_step[\"gini_personal_reward\"] = compute_gini_for_df(df_maze_step, prefix=\"reward_\")\n",
    "    print('succesfull for 2D step')\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute Gini for df_maze_step: {e}\")\n",
    "\n",
    "try:\n",
    "    df_gt_summary[\"gini_personal_reward\"] = compute_gini_for_df(df_gt_summary, prefix=\"total_personal_reward_\")\n",
    "    print('succesfull for GameTheoretic summary')\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute Gini for df_gt_summary: {e}\")\n",
    "\n",
    "try:\n",
    "    df_gt_step[\"gini_personal_reward\"] = compute_gini_for_df(df_gt_step, prefix=\"reward_\")\n",
    "    print('succesfull for GameTheoretic step')\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute Gini for df_gt_step: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2e70a3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully computed resource depletion from df_gt_step.\n",
      "Successfully computed resource depletion from df_gt_summary.\n",
      "Could not compute resource depletion from df_maze_step: 'simulation_index'\n",
      "Could not compute resource depletion from df_maze_summary: 'resource_remaining'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    GT_compute_and_merge_depletion_from_step(df_gt_step)  # creates \"depletion_final\" and \"depletion_cumulative\"\n",
    "    print(\"Successfully computed resource depletion from df_gt_step.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute resource depletion from df_gt_step: {e}\")\n",
    "\n",
    "try:\n",
    "    GT_compute_depletion_from_summary(df_gt_summary)  # creates \"depletion_cumulative\" and \"depletion_early\"\n",
    "    print(\"Successfully computed resource depletion from df_gt_summary.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute resource depletion from df_gt_summary: {e}\")\n",
    "\n",
    "# Will maybe need another formula for the 2D ?\n",
    "\n",
    "try:\n",
    "    GT_compute_and_merge_depletion_from_step(df_maze_step) \n",
    "    print(\"Successfully computed resource depletion from df_maze_step.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute resource depletion from df_maze_step: {e}\")\n",
    "\n",
    "try:\n",
    "    GT_compute_depletion_from_summary(df_maze_summary)\n",
    "    print(\"Successfully computed resource depletion from df_maze_summary.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute resource depletion from df_maze_summary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d88da0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_efficiency_for_df() missing 1 required positional argument: 'new_column_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[199], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m prefix_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersonal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m prefix_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_personal_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m compute_efficiency_for_df(df\u001b[38;5;241m=\u001b[39mdf_gt_step, prefix\u001b[38;5;241m=\u001b[39mprefix_step)\n\u001b[0;32m      5\u001b[0m compute_efficiency_for_df(df\u001b[38;5;241m=\u001b[39mdf_gt_summary, prefix\u001b[38;5;241m=\u001b[39mprefix_summary)\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_efficiency_for_df() missing 1 required positional argument: 'new_column_name'"
     ]
    }
   ],
   "source": [
    "# Compute the Efficiency of agents : set to the personal reward (dependent on ressource consumption and not the social reward)\n",
    "prefix_step = \"personal\"\n",
    "prefix_summary = \"total_personal_reward\"\n",
    "compute_efficiency_for_df(df=df_gt_step, prefix=prefix_step, new_column_name=f\"{prefix_step}_averaged_efficiency\")\n",
    "compute_efficiency_for_df(df=df_gt_summary, prefix=prefix_summary, new_column_name=f\"{prefix_summary}_averaged_efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347ba7a",
   "metadata": {},
   "source": [
    "## Learning verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac3de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_gt_summary\n",
    "\n",
    "df = df.merge(parameter_dataframe, on=\"simulation_index\", how=\"left\")\n",
    "\n",
    "# 2. Define which parameters define a unique experiment setting\n",
    "core_params = [col for col in parameter_dataframe.columns if col not in ['filepath', 'random_suffix', 'suffix']]\n",
    "\n",
    "# 3. Group by unique experimental configurations\n",
    "for param_values, group in df.groupby(core_params):\n",
    "    # Build readable label for logging\n",
    "    label = ', '.join(f\"{k}={v}\" for k, v in zip(core_params, param_values))\n",
    "\n",
    "    print(f\"\\n--- Processing Group: {label} ---\")\n",
    "\n",
    "    # 4. Call your existing function for each group\n",
    "    windowed_df = windowed_avg_combined_reward(\n",
    "        df=group,\n",
    "        reward_prefix=\"total_combined_reward_\",\n",
    "        episode_column=\"episode\",\n",
    "        simulation_id_column=\"simulation_index\",\n",
    "        window_size=10,\n",
    "        aggregation_mode=\"mean\",\n",
    "        plot=True  # Or False if you're running in batch\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53efc1c",
   "metadata": {},
   "source": [
    "## Data Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fluctuation of ending ressources per episode\n",
    "plot_value_fluctuation_by_simulation(df_gt_summary, value_col='resource_remaining', is_step_csv=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac97253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot density function of the proportion of depletion\n",
    "plot_depletion_density_across_simulations(df_gt_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e463fc1",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133a4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats : t-test to compare the empathic comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbdfed",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
